# -*- coding: utf-8 -*-
"""train_ner_mit_movie_corpus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jz2teFfrsQVf4zKutanCBu9K1TiHxPhh
"""

# -*- coding: utf-8 -*-
"""Fine-tuning BERT for NER (Named Entity Recognition)"""

# Install necessary libraries for Colab
!pip install transformers torch

# Import libraries
import csv
import os
import numpy as np
import pandas as pd
import torch
import random
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup
from sklearn.metrics import classification_report

# Download the MIT Movie Corpus files
!wget -q https://sls.csail.mit.edu/downloads/movie/engtrain.bio -O engtrain.bio
!wget -q https://sls.csail.mit.edu/downloads/movie/engtest.bio -O engtest.bio

print("Downloaded dataset files.")

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Helper function to load and process data
def get_sentences_and_labels(file_path):
    sentences = []  # Store tokenized words lists for each sentence
    labels = []     # Store labels for each word in a sentence
    unique_labels = set()  # Track unique labels

    # Temporary storage for the current sentence
    tokens = []
    token_labels = []

    with open(file_path, newline='', encoding='utf-8') as file:
        line_reader = csv.reader(file, delimiter='\t')

        for line in line_reader:
            # Ignore empty or improperly formatted lines
            if not line or len(line) < 2:
                if tokens:  # End of a sentence
                    sentences.append(tokens)
                    labels.append(token_labels)
                    tokens = []
                    token_labels = []
                continue

            # Extract token and label
            token, label = line[1], line[0]
            tokens.append(token)
            token_labels.append(label)
            unique_labels.add(label)

    # Append the last sentence if not empty
    if tokens:
        sentences.append(tokens)
        labels.append(token_labels)

    return sentences, labels, unique_labels

# Paths to the downloaded dataset files
train_file_path = 'engtrain.bio'
test_file_path = 'engtest.bio'

# Extract sentences, labels, and unique labels for train and test datasets
train_sentences, train_labels, unique_labels_train = get_sentences_and_labels(train_file_path)
test_sentences, test_labels, unique_labels_test = get_sentences_and_labels(test_file_path)

# Combine unique labels from train and test
unique_labels = unique_labels_train.union(unique_labels_test)

# Print dataset details
print(f"Number of sentences in training data: {len(train_sentences)}")
print(f"Number of sentences in testing data: {len(test_sentences)}")
print(f"Number of unique labels: {len(unique_labels)}")

# Create a label-to-ID mapping
label_map = {label: idx for idx, label in enumerate(sorted(unique_labels))}
print(f"Label Map: {label_map}")

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

# Prepare input IDs and attention masks
def get_input_ids_and_attention_masks(sentences):
    input_ids = []
    attention_masks = []
    for sentence in sentences:
        joined_sentence = ' '.join(sentence)
        encoded = tokenizer.encode_plus(
            joined_sentence,
            add_special_tokens=True,
            max_length=59,
            truncation=True,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt'
        )
        input_ids.append(encoded['input_ids'][0])
        attention_masks.append(encoded['attention_mask'][0])
    return input_ids, attention_masks

# Tokenize and encode
train_input_ids, train_attention_masks = get_input_ids_and_attention_masks(train_sentences)
test_input_ids, test_attention_masks = get_input_ids_and_attention_masks(test_sentences)

# Pad labels
def pad_labels(input_ids, labels, label_map):
    padded_labels = []
    for sent, orig_labels in zip(input_ids, labels):
        curr_labels = []
        label_idx = 0
        for token_id in sent:
            token_id = token_id.item()
            if token_id in {tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id}:
                curr_labels.append(-100)
            elif tokenizer.decode([token_id]).startswith("##"):
                curr_labels.append(-100)
            else:
                curr_labels.append(label_map[orig_labels[label_idx]])
                label_idx += 1
        assert len(sent) == len(curr_labels)
        padded_labels.append(curr_labels)
    return padded_labels

# Prepare labels
train_padded_labels = pad_labels(train_input_ids, train_labels, label_map)
test_padded_labels = pad_labels(test_input_ids, test_labels, label_map)

# Convert to tensors
train_input_ids_tensor = torch.stack(train_input_ids)
train_attention_masks_tensor = torch.stack(train_attention_masks)
train_padded_labels_tensor = torch.tensor(train_padded_labels)

test_input_ids_tensor = torch.stack(test_input_ids)
test_attention_masks_tensor = torch.stack(test_attention_masks)
test_padded_labels_tensor = torch.tensor(test_padded_labels)

# Split test set into validation and final test
val_size = int(0.2 * len(test_input_ids_tensor))
test_size = len(test_input_ids_tensor) - val_size

val_indices, final_test_indices = random_split(range(len(test_input_ids_tensor)), [val_size, test_size])

val_input_ids = test_input_ids_tensor[val_indices.indices]
val_attention_masks = test_attention_masks_tensor[val_indices.indices]
val_labels = test_padded_labels_tensor[val_indices.indices]

final_test_input_ids = test_input_ids_tensor[final_test_indices.indices]
final_test_attention_masks = test_attention_masks_tensor[final_test_indices.indices]
final_test_labels = test_padded_labels_tensor[final_test_indices.indices]

batch_size = 32

# Define training dataset and DataLoader
train_dataset = TensorDataset(train_input_ids_tensor, train_attention_masks_tensor, train_padded_labels_tensor)
train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)

# Validation DataLoader from the validation subset (20% of test data)
val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)
val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)

# Final test dataset and DataLoader
final_test_dataset = TensorDataset(final_test_input_ids, final_test_attention_masks, final_test_labels)
final_test_dataloader = DataLoader(final_test_dataset, sampler=SequentialSampler(final_test_dataset), batch_size=batch_size)

# Initialize the BERT model for token classification
model = BertForTokenClassification.from_pretrained(
    "bert-base-cased",
    num_labels=len(label_map) + 1  # Account for the padding label (-100)
)
model.to(device)

epochs = 4

# Optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)  # Standard learning rate and epsilon values
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

# Training loop
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

train_loss_values = []
test_results_per_epoch = []  # To store test evaluation results per epoch

for epoch in range(epochs):
    print(f"======== Epoch {epoch + 1} / {epochs} ========")

    # Training phase
    print("Training...")
    model.train()
    total_train_loss = 0
    for batch in train_dataloader:
        b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch]
        model.zero_grad()
        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs.loss
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
    avg_train_loss = total_train_loss / len(train_dataloader)
    train_loss_values.append(avg_train_loss)
    print(f"  Training loss: {avg_train_loss:.2f}")

    # Test evaluation after each epoch
    print("Evaluating on test set...")
    model.eval()
    predictions, true_labels = [], []
    with torch.no_grad():
        for batch in final_test_dataloader:
            b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch]
            outputs = model(b_input_ids, attention_mask=b_input_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=2).cpu().numpy()
            labels = b_labels.cpu().numpy()
            for pred, label in zip(preds, labels):
                predictions.append(pred[label != -100])
                true_labels.append(label[label != -100])
    # Flatten predictions and true labels
    flat_preds = [item for sublist in predictions for item in sublist]
    flat_true = [item for sublist in true_labels for item in sublist]

    # Remove padding labels (-100) from `label_map`
    label_names = [key for key, value in sorted(label_map.items(), key=lambda x: x[1]) if key != -100]

    # Generate classification report for test set
    report = classification_report(
        flat_true,
        flat_preds,
        target_names=label_names[:len(set(flat_true))],  # Ensure matching size
        output_dict=True
    )

    # Display report
    print(f"Test Results for Epoch {epoch + 1}:")
    print(classification_report(flat_true, flat_preds, target_names=label_names[:len(set(flat_true))]))

    # Store results for later analysis
    test_results_per_epoch.append(report)

# Plot training loss
plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), train_loss_values, label="Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Training Loss")
plt.show()

# Optional: Save the test results for all epochs
import pandas as pd
results_df = pd.DataFrame(test_results_per_epoch)
results_df.to_csv("test_results_per_epoch.csv", index=False)
print("Test results saved to test_results_per_epoch.csv")

# Testing the model
test_sentence = "Can you tell me the publication date of Tom Meets Zizou?"
encoded_test = tokenizer.encode_plus(
    test_sentence,
    add_special_tokens=True,
    max_length=59,
    truncation=True,
    padding='max_length',
    return_attention_mask=True,
    return_tensors='pt'
)

input_ids = encoded_test['input_ids'].to(device)
attention_mask = encoded_test['attention_mask'].to(device)

model.eval()
with torch.no_grad():
    outputs = model(input_ids, attention_mask=attention_mask)
    predictions = torch.argmax(outputs.logits, dim=2).cpu().numpy()

# Decode predictions
for token, label_id in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), predictions[0]):
    if label_id != -100:
        print(f"{token}: {list(label_map.keys())[list(label_map.values()).index(label_id)]}")

# # Define a function to evaluate the model on the final test set
# def evaluate_model(model, dataloader, label_map):
#     model.eval()
#     predictions, true_labels = [], []

#     with torch.no_grad():
#         for batch in dataloader:
#             b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch]
#             outputs = model(b_input_ids, attention_mask=b_input_mask)
#             logits = outputs.logits
#             preds = torch.argmax(logits, dim=2).cpu().numpy()
#             labels = b_labels.cpu().numpy()

#             for pred, label in zip(preds, labels):
#                 predictions.append(pred[label != -100])  # Filter out padding labels
#                 true_labels.append(label[label != -100])

#     flat_preds = [label for sublist in predictions for label in sublist]
#     flat_labels = [label for sublist in true_labels for label in sublist]
#     return classification_report(flat_labels, flat_preds)

# # Evaluate the model
# print(evaluate_model(model, final_test_dataloader, label_map))

# import os

# # Define a directory to save the model
# output_dir = "./fine_tuned_bert_model"

# # Create the directory if it doesn't exist
# if not os.path.exists(output_dir):
#     os.makedirs(output_dir)

# # Save the model
# model.save_pretrained(output_dir)

# # Save the tokenizer
# tokenizer.save_pretrained(output_dir)

# print(f"Model and tokenizer saved to {output_dir}")

files.download('fine_tuned_bert_model.zip')

from google.colab import drive
drive.mount('/content/drive')

import shutil

# Define the path in Google Drive where you want to save the zip file
drive_path = '/content/drive/My Drive/fine_tuned_bert_model'

# Zip the model directory and save it to Google Drive
shutil.make_archive(drive_path, 'zip', './fine_tuned_bert_model')

print(f"Model saved to {drive_path}.zip")